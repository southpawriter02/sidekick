# v1.2.1 — Rate-Limited LLM Requests

**Released:** 2026-02-07

## Summary

All LLM requests now pass through a sliding-window rate limiter with
exponential back-off. This prevents agent self-correction loops from
saturating the local LLM.

## New Features

### Request Rate Limiting

- Sliding-window token bucket: configurable requests-per-minute cap
- Exponential back-off on limit hits (`base × 2^n`, capped at `maxDelayMs`)
- Thread-safe via `ReentrantLock` — safe for concurrent agent tasks
- Automatic window expiry resets the counter each minute
- Runtime config updates without restart via `updateConfig()`

### Built-In Presets

| Preset | Requests/Min | Back-Off Base | Max Delay |
|---|---|---|---|
| `DEFAULT` | 60 | 1 000 ms | 30 s |
| `CONSERVATIVE` | 20 | 2 000 ms | 60 s |
| `STRICT` | 10 | 3 000 ms | 120 s |
| `DISABLED` | `MAX_VALUE` | 0 ms | 0 ms |

### Provider Integration

- `ProviderManager.chat()`, `streamChat()`, and `embed()` call
  `rateLimiter.acquire()` before delegating to the backend
- `getRateLimitStats()` exposes current window usage
- `resetRateLimiter()` clears state for testing or manual recovery

## Files Changed

| File | Change |
|---|---|
| `RateLimiter.kt` | **NEW** — Sliding-window token bucket with back-off |
| `ProviderManager.kt` | Integrated rate limiter into all LLM request paths |
| `RateLimiterTest.kt` | **NEW** — 25+ unit tests covering limits, back-off, config |
